{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c757126c",
   "metadata": {},
   "source": [
    "# 本地化大模型部署与验证\n",
    "\n",
    "本 Notebook 包含以下步骤：\n",
    "1. **环境检查**: 确认 GPU 和 CUDA 环境是否满足要求。\n",
    "2. **模型下载**: 从 ModelScope 下载 Qwen2.5-7B-Instruct-GPTQ-Int4 量化模型。\n",
    "3. **模型加载**: 加载 Tokenizer 和模型到显存。\n",
    "4. **推理测试**: 定义对话函数并进行测试。\n",
    "5. **多模态模型**: 下载并测试 Qwen3-VL-8B-Instruct-4bit-GPTQ (视觉语言模型)。\n",
    "\n",
    "**参考文章**：[解决 AutoGPTQ 推理慢的问题](https://jishuzhan.net/article/1868560413910634498)\n",
    "\n",
    "**关键问题解决记录**：\n",
    "- **问题**：初始部署推理延迟极高（>40秒）\n",
    "- **根本原因**：AutoGPTQ 0.7.1 需要 PyTorch 2.2.1+，而环境中是 PyTorch 2.1.2，导致 CUDA 扩展未正确加载，回退到 CPU 执行。\n",
    "- **解决方案**：降级到 AutoGPTQ 0.6.0（适配 PyTorch 2.1.2）。\n",
    "- **结果**：性能提升 12 倍（推理时间从 43.7s 降至 3.5s）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8c53c6",
   "metadata": {},
   "source": [
    "## 1. conda环境配置与环境检查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afa4d8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "! conda env create -f environment.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce87c3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.10.19 | packaged by conda-forge | (main, Oct 22 2025, 22:23:22) [MSC v.1944 64 bit (AMD64)]\n",
      "PyTorch Version: 2.1.2+cu121\n",
      "CUDA is available!\n",
      "GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "CUDA Version: 12.1\n",
      "Total VRAM: 8.00 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available!\")\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    # Check VRAM\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"Total VRAM: {total_memory:.2f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: CUDA is NOT available. Inference will be extremely slow on CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d78e2a",
   "metadata": {},
   "source": [
    "## 2. 模型下载\n",
    "\n",
    "使用 **Qwen2.5-7B-Instruct-GPTQ-Int4** 量化模型：\n",
    "- 模型大小：约 4.3GB（Int4 量化）\n",
    "\n",
    "- 显存占用：约 5-6GB- 结果：性能提升 12 倍\n",
    "\n",
    "- 推理速度：约 3-4 秒/次（修复后）- 解决：AutoGPTQ 0.7.1 → 0.6.0，匹配 PyTorch 2.1.2\n",
    "\n",
    "- 适配硬件：RTX 4060 8GB- 原因：AutoGPTQ CUDA 扩展未加载（版本不匹配）\n",
    "\n",
    "- 问题：初始部署推理延迟 >40 秒\n",
    "**性能优化记录**："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "946b2a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始下载 Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4 模型...\n",
      "Downloading Model from https://www.modelscope.cn to directory: ./models\\Qwen\\Qwen2.5-7B-Instruct-GPTQ-Int4\n",
      "Downloading Model from https://www.modelscope.cn to directory: ./models\\Qwen\\Qwen2.5-7B-Instruct-GPTQ-Int4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 20:35:50,628 - modelscope - INFO - Creating symbolic link [./models\\Qwen\\Qwen2.5-7B-Instruct-GPTQ-Int4].\n",
      "2025-12-14 20:35:50,638 - modelscope - WARNING - Failed to create symbolic link ./models\\Qwen\\Qwen2.5-7B-Instruct-GPTQ-Int4 for d:\\llm_deploy\\models\\Qwen\\Qwen2___5-7B-Instruct-GPTQ-Int4.\n",
      "2025-12-14 20:35:50,638 - modelscope - WARNING - Failed to create symbolic link ./models\\Qwen\\Qwen2.5-7B-Instruct-GPTQ-Int4 for d:\\llm_deploy\\models\\Qwen\\Qwen2___5-7B-Instruct-GPTQ-Int4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型下载成功！存储路径: ./models\\Qwen\\Qwen2___5-7B-Instruct-GPTQ-Int4\n"
     ]
    }
   ],
   "source": [
    "from modelscope import snapshot_download\n",
    "import os\n",
    "\n",
    "# 确保下载目录存在\n",
    "os.makedirs('./models', exist_ok=True)\n",
    "\n",
    "# 下载 Qwen2.5-7B GPTQ Int4 模型\n",
    "model_id = 'Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4'\n",
    "\n",
    "print(f\"开始下载 {model_id} 模型...\")\n",
    "try:\n",
    "    model_dir = snapshot_download(\n",
    "        model_id, \n",
    "        cache_dir='./models',\n",
    "        revision='master'\n",
    "    )\n",
    "    print(f\"模型下载成功！存储路径: {model_dir}\")\n",
    "except Exception as e:\n",
    "    print(f\"模型下载失败: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29462589",
   "metadata": {},
   "source": [
    "## 3. 模型加载\n",
    "加载下载的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95025e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: ./models\\Qwen\\Qwen2.5-7B-Instruct-GPTQ-Int4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 20:35:54,300 - modelscope - INFO - Creating symbolic link [./models\\Qwen\\Qwen2.5-7B-Instruct-GPTQ-Int4].\n",
      "2025-12-14 20:35:54,313 - modelscope - WARNING - Failed to create symbolic link ./models\\Qwen\\Qwen2.5-7B-Instruct-GPTQ-Int4 for d:\\llm_deploy\\models\\Qwen\\Qwen2___5-7B-Instruct-GPTQ-Int4.\n",
      "2025-12-14 20:35:54,313 - modelscope - WARNING - Failed to create symbolic link ./models\\Qwen\\Qwen2.5-7B-Instruct-GPTQ-Int4 for d:\\llm_deploy\\models\\Qwen\\Qwen2___5-7B-Instruct-GPTQ-Int4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载 Tokenizer 和 模型 (Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ac8d0540724472ebd6f30389b72ea3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型加载完成！\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from modelscope import snapshot_download\n",
    "import torch\n",
    "\n",
    "model_id = 'Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4'\n",
    "\n",
    "# 获取模型路径\n",
    "model_dir = snapshot_download(model_id, cache_dir='./models')\n",
    "\n",
    "print(f\"正在加载 Tokenizer 和 模型 ({model_id})...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "\n",
    "# 加载 GPTQ 量化模型 (AutoGPTQ 0.6.0 会自动处理)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(\"模型加载完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16306246",
   "metadata": {},
   "source": [
    "## 4. 定义推理函数\n",
    "定义一个 `chat` 函数，封装 Prompt 构建和生成过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8299ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(query):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        top_p=0.8,\n",
    "        top_k=20,\n",
    "        do_sample=True\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0b4922",
   "metadata": {},
   "source": [
    "## 5. 测试对话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d6ad9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User: 你好，请介绍一下你自己。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: 你好！我是一个来自阿里云的语言模型，我叫通义千问。我可以帮助用户生成各种类型的文本，例如文章、故事、诗歌、故事等，并能够回答各种问题。尽管我非常强大，但我也不是完美无缺的，我还需要不断学习和进步。如果你有任何问题或需要帮助，都可以随时向我提问。\n"
     ]
    }
   ],
   "source": [
    "test_query = \"你好，请介绍一下你自己。\"\n",
    "print(f\"\\nUser: {test_query}\")\n",
    "# chat 函数已经返回了字符串，直接打印即可，不需要再用 print 包裹\n",
    "response = chat(test_query)\n",
    "print(f\"Assistant: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8dafe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User: 鲁迅和周树人是什么关系？\n",
      "Assistant: 鲁迅是周树人的笔名。鲁迅是中国近现代著名的文学家、思想家、革命家，原名周树人，浙江绍兴人。1918年5月，首次用“鲁迅”作为自己的笔名，发表中国现代文学史上第一篇白话文小说《狂人日记》。\n",
      "Assistant: 鲁迅是周树人的笔名。鲁迅是中国近现代著名的文学家、思想家、革命家，原名周树人，浙江绍兴人。1918年5月，首次用“鲁迅”作为自己的笔名，发表中国现代文学史上第一篇白话文小说《狂人日记》。\n"
     ]
    }
   ],
   "source": [
    "# 使用经典问题来测试下\n",
    "test_query_2 = \"鲁迅和周树人是什么关系？\"\n",
    "print(f\"\\nUser: {test_query_2}\")\n",
    "response_2 = chat(test_query_2)\n",
    "print(f\"Assistant: {response_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c3eb8d",
   "metadata": {},
   "source": [
    "## 6. 下载 Qwen3-VL-8B 多模态模型（4-bit 量化版）\n",
    "\n",
    "尝试下载 Qwen3-VL-8B-Instruct-4bit-GPTQ 量化模型（支持图像理解，显存占用约 6-8GB）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e375258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始下载 DavidWen2025/Qwen3-VL-8B-Instruct-4bit-GPTQ 多模态模型...\n",
      "下载路径: d:\\llm_deploy\\LLM\\LLM_DEPLOY\\models\n",
      "Downloading Model from https://www.modelscope.cn to directory: ./models\\DavidWen2025\\Qwen3-VL-8B-Instruct-4bit-GPTQ\n",
      "Downloading Model from https://www.modelscope.cn to directory: ./models\\DavidWen2025\\Qwen3-VL-8B-Instruct-4bit-GPTQ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 21:59:37,018 - modelscope - INFO - Got 2 files, start to download ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "727044e81ad3425e8585eb141bd2beb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing 2 items:   0%|          | 0.00/2.00 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "875421ebc92a4138909e4c7f035819b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [model-00001-of-00002.safetensors]:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b34ae11d28b4784b300e530712054d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [model-00002-of-00002.safetensors]:   0%|          | 0.00/2.76G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 22:33:48,197 - modelscope - INFO - Download model 'DavidWen2025/Qwen3-VL-8B-Instruct-4bit-GPTQ' successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型下载成功！存储路径: ./models\\DavidWen2025\\Qwen3-VL-8B-Instruct-4bit-GPTQ\n"
     ]
    }
   ],
   "source": [
    "from modelscope import snapshot_download\n",
    "import os\n",
    "\n",
    "# 使用相对路径存储在 notebook 所在目录\n",
    "cache_dir = './models'\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "# 下载 Qwen3-VL-8B 4-bit GPTQ 量化模型（适合 8GB 显存）\n",
    "vl_model_id = 'DavidWen2025/Qwen3-VL-8B-Instruct-4bit-GPTQ'\n",
    "\n",
    "print(f\"开始下载 {vl_model_id} 多模态模型...\")\n",
    "print(f\"下载路径: {os.path.abspath(cache_dir)}\")\n",
    "\n",
    "try:\n",
    "    vl_model_dir = snapshot_download(\n",
    "        vl_model_id,\n",
    "        cache_dir=cache_dir,\n",
    "        revision='master'\n",
    "    )\n",
    "    print(f\"模型下载成功！存储路径: {vl_model_dir}\")\n",
    "except Exception as e:\n",
    "    print(f\"模型下载失败: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c13a2b6",
   "metadata": {},
   "source": [
    "### 6.1 加载并测试 Qwen3-VL-8B 4-bit 量化模型\n",
    "\n",
    "加载 4-bit 量化的视觉语言模型，测试文本和图像理解能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c75c688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: ./models\\DavidWen2025\\Qwen3-VL-8B-Instruct-4bit-GPTQ\n",
      "正在加载 Qwen3-VL-8B 4-bit GPTQ 模型...\n",
      "模型路径: ./models\\DavidWen2025\\Qwen3-VL-8B-Instruct-4bit-GPTQ\n",
      "Config already patched.\n",
      "Converting text_config dict to Config object...\n",
      "❌ 模型加载失败: type object 'AutoConfig' has no attribute 'from_dict'\n",
      "正在加载 Qwen3-VL-8B 4-bit GPTQ 模型...\n",
      "模型路径: ./models\\DavidWen2025\\Qwen3-VL-8B-Instruct-4bit-GPTQ\n",
      "Config already patched.\n",
      "Converting text_config dict to Config object...\n",
      "❌ 模型加载失败: type object 'AutoConfig' has no attribute 'from_dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Iron\\AppData\\Local\\Temp\\ipykernel_12780\\3922773060.py\", line 69, in <module>\n",
      "    config.text_config = AutoConfig.from_dict(config.text_config)\n",
      "AttributeError: type object 'AutoConfig' has no attribute 'from_dict'\n"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor, AutoConfig\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from modelscope import snapshot_download\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "\n",
    "# 获取模型路径\n",
    "vl_model_id = 'DavidWen2025/Qwen3-VL-8B-Instruct-4bit-GPTQ'\n",
    "vl_model_dir = snapshot_download(vl_model_id, cache_dir='./models')\n",
    "\n",
    "print(f\"正在加载 Qwen3-VL-8B 4-bit GPTQ 模型...\")\n",
    "print(f\"模型路径: {vl_model_dir}\")\n",
    "\n",
    "# 修复配置文件中的兼容性问题 (Qwen3-VL -> Qwen2-VL)\n",
    "config_path = os.path.join(vl_model_dir, 'config.json')\n",
    "with open(config_path, 'r', encoding='utf-8') as f:\n",
    "    config_dict = json.load(f)\n",
    "\n",
    "modified = False\n",
    "\n",
    "# 1. 修正主 model_type\n",
    "if config_dict.get('model_type') == 'qwen3_vl':\n",
    "    print(\"Fixing model_type: qwen3_vl -> qwen2_vl\")\n",
    "    config_dict['model_type'] = 'qwen2_vl'\n",
    "    modified = True\n",
    "\n",
    "# 2. 修正 architectures\n",
    "if 'Qwen3VLForConditionalGeneration' in config_dict.get('architectures', []):\n",
    "    print(\"Fixing architectures: Qwen3VL -> Qwen2VL\")\n",
    "    config_dict['architectures'] = ['Qwen2VLForConditionalGeneration']\n",
    "    modified = True\n",
    "\n",
    "# 3. 修正 text_config\n",
    "if 'text_config' in config_dict and isinstance(config_dict['text_config'], dict):\n",
    "    if config_dict['text_config'].get('model_type') == 'qwen3_vl_text':\n",
    "        print(\"Fixing text_config model_type: qwen3_vl_text -> qwen2\")\n",
    "        config_dict['text_config']['model_type'] = 'qwen2'\n",
    "        modified = True\n",
    "    \n",
    "    if '_name_or_path' not in config_dict['text_config']:\n",
    "        config_dict['text_config']['_name_or_path'] = ''\n",
    "        modified = True\n",
    "\n",
    "# 4. 修正 vision_config\n",
    "if 'vision_config' in config_dict and isinstance(config_dict['vision_config'], dict):\n",
    "    if config_dict['vision_config'].get('model_type') == 'qwen3_vl':\n",
    "        print(\"Fixing vision_config model_type: qwen3_vl -> qwen2_vl\")\n",
    "        config_dict['vision_config']['model_type'] = 'qwen2_vl'\n",
    "        modified = True\n",
    "\n",
    "# 保存修复后的配置\n",
    "if modified:\n",
    "    print(\"Saving patched config.json...\")\n",
    "    with open(config_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(config_dict, f, indent=2, ensure_ascii=False)\n",
    "else:\n",
    "    print(\"Config already patched.\")\n",
    "\n",
    "# 加载模型和处理器（使用 4-bit 量化）\n",
    "try:\n",
    "    # 显式加载 Config 对象\n",
    "    config = AutoConfig.from_pretrained(vl_model_dir, trust_remote_code=True)\n",
    "    \n",
    "    # 手动将 text_config 和 vision_config 转换为 Config 对象\n",
    "    # 解决 AttributeError: 'dict' object has no attribute 'to_dict'\n",
    "    if hasattr(config, 'text_config') and isinstance(config.text_config, dict):\n",
    "        print(\"Converting text_config dict to Config object...\")\n",
    "        config.text_config = AutoConfig.from_dict(config.text_config)\n",
    "        \n",
    "    if hasattr(config, 'vision_config') and isinstance(config.vision_config, dict):\n",
    "        print(\"Converting vision_config dict to Config object...\")\n",
    "        config.vision_config = AutoConfig.from_dict(config.vision_config)\n",
    "\n",
    "    vl_model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "        vl_model_dir,\n",
    "        config=config, # 传入修复后的 config 对象\n",
    "        device_map={\"\": 0} if torch.cuda.is_available() else {\"\": \"cpu\"},\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    vl_processor = AutoProcessor.from_pretrained(vl_model_dir, trust_remote_code=True)\n",
    "    \n",
    "    print(\"✅ Qwen3-VL-8B 4-bit 模型加载完成！\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ 模型加载失败: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc91125",
   "metadata": {},
   "source": [
    "### 6.2 测试纯文本对话\n",
    "\n",
    "先测试视觉模型的文本理解能力（不使用图像）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "601c42ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: 请用一句话介绍一下你自己的能力\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'vl_processor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 55\u001b[0m\n\u001b[0;32m     53\u001b[0m test_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m请用一句话介绍一下你自己的能力\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_query\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 55\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mvl_chat_text_only\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_query\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssistant: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[16], line 13\u001b[0m, in \u001b[0;36mvl_chat_text_only\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m      3\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      4\u001b[0m     {\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     }\n\u001b[0;32m     10\u001b[0m ]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# 应用聊天模板\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[43mvl_processor\u001b[49m\u001b[38;5;241m.\u001b[39mapply_chat_template(\n\u001b[0;32m     14\u001b[0m     messages, \n\u001b[0;32m     15\u001b[0m     tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[0;32m     16\u001b[0m     add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     17\u001b[0m )\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# 处理输入\u001b[39;00m\n\u001b[0;32m     20\u001b[0m image_inputs, video_inputs \u001b[38;5;241m=\u001b[39m process_vision_info(messages)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vl_processor' is not defined"
     ]
    }
   ],
   "source": [
    "def vl_chat_text_only(query):\n",
    "    \"\"\"纯文本对话（不使用图像）\"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": query}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # 应用聊天模板\n",
    "    text = vl_processor.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # 处理输入\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = vl_processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    inputs = inputs.to(vl_model.device)\n",
    "    \n",
    "    # 生成回答\n",
    "    generated_ids = vl_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        top_p=0.8,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):] \n",
    "        for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    \n",
    "    response = vl_processor.batch_decode(\n",
    "        generated_ids_trimmed,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0]\n",
    "    \n",
    "    return response\n",
    "\n",
    "# 测试纯文本对话\n",
    "test_query = \"请用一句话介绍一下你自己的能力\"\n",
    "print(f\"User: {test_query}\")\n",
    "response = vl_chat_text_only(test_query)\n",
    "print(f\"Assistant: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56a57c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6.3 测试图像理解（使用网络图片）\n",
    "\n",
    "测试模型的视觉理解能力，使用在线图片URL。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edb4e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vl_chat_with_image(query, image_url):\n",
    "    \"\"\"带图像的对话\"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image_url},\n",
    "                {\"type\": \"text\", \"text\": query}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # 应用聊天模板\n",
    "    text = vl_processor.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # 处理输入（包含图像）\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = vl_processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    inputs = inputs.to(vl_model.device)\n",
    "    \n",
    "    # 生成回答\n",
    "    generated_ids = vl_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        top_p=0.8,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):] \n",
    "        for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    \n",
    "    response = vl_processor.batch_decode(\n",
    "        generated_ids_trimmed,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0]\n",
    "    \n",
    "    return response\n",
    "\n",
    "# 测试图像理解\n",
    "image_url = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\"\n",
    "query = \"请详细描述这张图片的内容\"\n",
    "\n",
    "print(f\"图片URL: {image_url}\")\n",
    "print(f\"User: {query}\")\n",
    "print(\"\\\\n处理中...\")\n",
    "\n",
    "response = vl_chat_with_image(query, image_url)\n",
    "print(f\"\\\\nAssistant: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b56613",
   "metadata": {},
   "source": [
    "### 6.4 多轮对话测试\n",
    "\n",
    "测试带图像的多轮对话能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6495c941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多轮对话示例\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\"},\n",
    "            {\"type\": \"text\", \"text\": \"图片中有什么动物？\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# 第一轮\n",
    "text = vl_processor.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
    "image_inputs, video_inputs = process_vision_info(conversation)\n",
    "inputs = vl_processor(text=[text], images=image_inputs, videos=video_inputs, padding=True, return_tensors=\"pt\")\n",
    "inputs = inputs.to(vl_model.device)\n",
    "\n",
    "generated_ids = vl_model.generate(**inputs, max_new_tokens=256, temperature=0.7, do_sample=True)\n",
    "generated_ids_trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n",
    "response_1 = vl_processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "print(\"第一轮对话：\")\n",
    "print(f\"User: 图片中有什么动物？\")\n",
    "print(f\"Assistant: {response_1}\")\n",
    "\n",
    "# 添加第一轮回答到对话历史\n",
    "conversation.append({\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": response_1}]})\n",
    "\n",
    "# 第二轮（追问）\n",
    "conversation.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\"type\": \"text\", \"text\": \"它在做什么？\"}]\n",
    "})\n",
    "\n",
    "text = vl_processor.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
    "image_inputs, video_inputs = process_vision_info(conversation)\n",
    "inputs = vl_processor(text=[text], images=image_inputs, videos=video_inputs, padding=True, return_tensors=\"pt\")\n",
    "inputs = inputs.to(vl_model.device)\n",
    "\n",
    "generated_ids = vl_model.generate(**inputs, max_new_tokens=256, temperature=0.7, do_sample=True)\n",
    "generated_ids_trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n",
    "response_2 = vl_processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "print(f\"\\\\n第二轮对话：\")\n",
    "print(f\"User: 它在做什么？\")\n",
    "print(f\"Assistant: {response_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c949cf",
   "metadata": {},
   "source": [
    "### 6.5 测试本地图片与总结\n",
    "\n",
    "如果有本地图片，也可以这样使用：\n",
    "\n",
    "```python\n",
    "# 使用本地图片路径\n",
    "local_image_path = \"./test_image.jpg\"\n",
    "query = \"描述这张图片\"\n",
    "response = vl_chat_with_image(query, f\"file://{os.path.abspath(local_image_path)}\")\n",
    "print(response)\n",
    "```\n",
    "\n",
    "### 注意事项\n",
    "\n",
    "1. **显存需求**：Qwen3-VL-8B 4-bit GPTQ 需要约 6-8GB 显存，**适合 RTX 4060 8GB**\n",
    "2. **量化效果**：4-bit 量化在保持性能的同时大幅降低显存占用\n",
    "3. **图片格式**：支持 JPEG、PNG 等常见格式\n",
    "4. **视频支持**：Qwen3-VL 还支持视频输入（使用 `{\"type\": \"video\", \"video\": \"path\"}`)\n",
    "\n",
    "## 总结\n",
    "\n",
    "本 Notebook 演示了：\n",
    "- ✅ Qwen2.5-7B 文本模型的部署与测试（Int4 量化，3.5秒推理）\n",
    "- ✅ Qwen3-VL-8B 4-bit GPTQ 多模态模型的加载\n",
    "- ✅ 纯文本对话功能\n",
    "- ✅ 图像理解功能\n",
    "- ✅ 多轮对话功能\n",
    "\n",
    "**模型对比**：\n",
    "| 模型 | 大小 | 显存占用 | 适配硬件 | 功能 |\n",
    "|------|------|---------|---------|------|\n",
    "| Qwen2.5-7B-GPTQ-Int4 | 4.3GB | 5-6GB | RTX 4060 8GB ✅ | 纯文本 |\n",
    "| Qwen3-VL-8B-4bit-GPTQ | 5-6GB | 6-8GB | RTX 4060 8GB ✅ | 文本+图像 |\n",
    "\n",
    "**下一步**：\n",
    "- 运行 `02_rag_implementation.ipynb` 构建 RAG 知识库\n",
    "- 启动 `rag_server.py` 部署 RAG API 服务"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm_deploy)",
   "language": "python",
   "name": "llm_deploy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
