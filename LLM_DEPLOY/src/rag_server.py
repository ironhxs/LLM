from flask import Flask, request, jsonify
from flask_cors import CORS
from transformers import AutoTokenizer, AutoModelForCausalLM
try:
    from langchain_huggingface import HuggingFaceEmbeddings
except ImportError:
    from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from modelscope import snapshot_download
import os
import time

app = Flask(__name__)
CORS(app)

# å…¨å±€å˜é‡
tokenizer = None
model = None
vectorstore = None
embedding_model = None

def load_models():
    """åŠ è½½ LLM å’Œå‘é‡æ•°æ®åº“"""
    global tokenizer, model, vectorstore, embedding_model
    
    # è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆsrcçš„ä¸Šçº§ç›®å½•ï¼‰
    BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    MODELS_DIR = os.path.join(BASE_DIR, "models")
    VECTOR_STORE_PATH = os.path.join(BASE_DIR, "vector_store")
    
    print("=" * 50)
    print("æ­£åœ¨åˆå§‹åŒ– RAG ç³»ç»Ÿ...")
    print(f"å·¥ä½œç›®å½•: {BASE_DIR}")
    print("=" * 50)
    
    # 1. åŠ è½½ Embedding æ¨¡å‹
    print("\n[1/3] åŠ è½½ Embedding æ¨¡å‹...")
    # å°è¯•ä»æœ¬åœ° models ç›®å½•åŠ è½½ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä¸‹è½½
    embedding_model_id = 'AI-ModelScope/text2vec-base-chinese'
    try:
        print(f"æ­£åœ¨æ£€æŸ¥/ä¸‹è½½ Embedding æ¨¡å‹åˆ°: {MODELS_DIR}")
        embedding_model_path = snapshot_download(embedding_model_id, cache_dir=MODELS_DIR)
        print(f"ä½¿ç”¨æœ¬åœ° Embedding æ¨¡å‹: {embedding_model_path}")
    except Exception as e:
        print(f"ModelScope ä¸‹è½½å¤±è´¥ï¼Œå°è¯•åœ¨çº¿åŠ è½½: {e}")
        embedding_model_path = "shibing624/text2vec-base-chinese"

    embedding_model = HuggingFaceEmbeddings(
        model_name=embedding_model_path,
        model_kwargs={'device': 'cpu'}, # æ¨ç†æœåŠ¡é€šå¸¸æ˜¾å­˜ç´§å¼ ï¼ŒEmbedding ç”¨ CPU å³å¯
        encode_kwargs={'normalize_embeddings': True}
    )
    print("âœ… Embedding æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # 2. åŠ è½½ FAISS å‘é‡åº“
    print("\n[2/3] åŠ è½½ FAISS å‘é‡æ•°æ®åº“...")
    print(f"å‘é‡åº“è·¯å¾„: {VECTOR_STORE_PATH}")
    
    if not os.path.exists(VECTOR_STORE_PATH):
        print("âŒ é”™è¯¯: å‘é‡æ•°æ®åº“ä¸å­˜åœ¨ï¼")
        print(f"è¯·å…ˆè¿è¡Œ 02_rag_implementation.ipynb æ„å»ºå‘é‡åº“")
        raise FileNotFoundError(f"å‘é‡åº“è·¯å¾„ä¸å­˜åœ¨: {VECTOR_STORE_PATH}")
    
    vectorstore = FAISS.load_local(
        VECTOR_STORE_PATH,
        embedding_model,
        allow_dangerous_deserialization=True
    )
    print(f"âœ… å‘é‡æ•°æ®åº“åŠ è½½å®Œæˆï¼ŒåŒ…å« {vectorstore.index.ntotal} ä¸ªå‘é‡")
    
    # 3. åŠ è½½ Qwen æ¨¡å‹
    print("\n[3/3] åŠ è½½ Qwen2.5-7B æ¨¡å‹...")
    model_id = 'Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4'
    # ä½¿ç”¨ç»å¯¹è·¯å¾„çš„ models ç›®å½•
    model_dir = snapshot_download(model_id, cache_dir=MODELS_DIR)
    
    tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        model_dir,
        device_map="auto",
        trust_remote_code=True
    )
    print("âœ… Qwen æ¨¡å‹åŠ è½½å®Œæˆ")
    
    print("\n" + "=" * 50)
    print("ğŸš€ RAG ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")
    print("=" * 50)

@app.route('/health', methods=['GET'])
def health():
    """å¥åº·æ£€æŸ¥"""
    return jsonify({
        "status": "healthy",
        "vectorstore_loaded": vectorstore is not None,
        "model_loaded": model is not None,
        "vector_count": vectorstore.index.ntotal if vectorstore else 0
    })

@app.route('/rag_chat', methods=['POST'])
def rag_chat():
    """RAG å¢å¼ºé—®ç­”"""
    try:
        data = request.json
        query = data.get('query', '')
        history = data.get('history', [])
        k = data.get('k', 3)  # æ£€ç´¢æ–‡æ¡£æ•°é‡
        
        if not query:
            return jsonify({"error": "æŸ¥è¯¢å†…å®¹ä¸èƒ½ä¸ºç©º"}), 400
        
        start_time = time.time()
        
        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        retrieved_docs = vectorstore.similarity_search(query, k=k)
        
        # 2. æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []
        sources = []
        
        for i, doc in enumerate(retrieved_docs):
            source_file = os.path.basename(doc.metadata.get('source', 'unknown'))
            sources.append({
                "index": i + 1,
                "source": source_file,
                "content": doc.page_content
            })
            context_parts.append(f"ã€å‚è€ƒèµ„æ–™ {i+1}ã€‘æ¥æº: {source_file}\n{doc.page_content}")
        
        context = "\n\n".join(context_parts)
        
        # 3. æ„å»ºæç¤ºè¯
        prompt = f"""ä½ æ˜¯åˆè‚¥å·¥ä¸šå¤§å­¦äººå·¥æ™ºèƒ½è¯¾ç¨‹çš„æ™ºèƒ½åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹å‚è€ƒèµ„æ–™å›ç­”å­¦ç”Ÿçš„é—®é¢˜ã€‚

è¦æ±‚ï¼š
1. å¦‚æœå‚è€ƒèµ„æ–™ä¸­æœ‰ç›¸å…³ä¿¡æ¯ï¼ŒåŸºäºèµ„æ–™ç»™å‡ºå‡†ç¡®ã€è¯¦ç»†çš„å›ç­”
2. å¦‚æœå‚è€ƒèµ„æ–™ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œæ˜ç¡®è¯´æ˜"å‚è€ƒèµ„æ–™ä¸­æš‚æ— ç›¸å…³å†…å®¹"
3. å›ç­”è¦ä¸“ä¸šã€æ¸…æ™°ã€æ¡ç†åˆ†æ˜
4. å¯¹äºè¯¾ç¨‹ã€å®éªŒç›¸å…³é—®é¢˜ï¼Œå°½å¯èƒ½ç»™å‡ºå…·ä½“æŒ‡å¯¼

{context}

ã€é—®é¢˜ã€‘
{query}

ã€å›ç­”ã€‘"""

        # 4. æ„å»ºå¯¹è¯å†å²
        messages = [{"role": "system", "content": "ä½ æ˜¯åˆè‚¥å·¥ä¸šå¤§å­¦äººå·¥æ™ºèƒ½è¯¾ç¨‹çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œä¸“é—¨ä¸ºå­¦ç”Ÿè§£ç­”è¯¾ç¨‹ç›¸å…³é—®é¢˜ã€‚"}]
        
        # æ·»åŠ å†å²å¯¹è¯ï¼ˆæœ€å¤šä¿ç•™æœ€è¿‘ 5 è½®ï¼‰
        for h in history[-5:]:
            messages.append({"role": "user", "content": h[0]})
            messages.append({"role": "assistant", "content": h[1]})
        
        messages.append({"role": "user", "content": prompt})
        
        # 5. è°ƒç”¨æ¨¡å‹ç”Ÿæˆ
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
        
        generated_ids = model.generate(
            model_inputs.input_ids,
            max_new_tokens=512,
            temperature=0.7,
            top_p=0.8,
            do_sample=True
        )
        
        generated_ids = [
            output_ids[len(input_ids):] 
            for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
        ]
        
        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        
        # 6. æ›´æ–°å†å²
        history.append([query, response])
        
        elapsed = time.time() - start_time
        
        return jsonify({
            "response": response,
            "sources": sources,
            "history": history,
            "elapsed_time": round(elapsed, 2)
        })
        
    except Exception as e:
        print(f"é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500

@app.route('/chat', methods=['POST'])
def simple_chat():
    """æ™®é€šå¯¹è¯ï¼ˆä¸ä½¿ç”¨ RAGï¼‰"""
    try:
        data = request.json
        query = data.get('query', '')
        history = data.get('history', [])
        
        if not query:
            return jsonify({"error": "æŸ¥è¯¢å†…å®¹ä¸èƒ½ä¸ºç©º"}), 400
        
        start_time = time.time()
        
        # æ„å»ºå¯¹è¯
        messages = [{"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œæ“…é•¿å›ç­”æŠ€æœ¯å’Œå­¦æœ¯ç›¸å…³é—®é¢˜ã€‚"}]
        
        for h in history[-5:]:
            messages.append({"role": "user", "content": h[0]})
            messages.append({"role": "assistant", "content": h[1]})
        
        messages.append({"role": "user", "content": query})
        
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
        
        generated_ids = model.generate(
            model_inputs.input_ids,
            max_new_tokens=512,
            temperature=0.7,
            top_p=0.8,
            do_sample=True
        )
        
        generated_ids = [
            output_ids[len(input_ids):] 
            for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
        ]
        
        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        
        history.append([query, response])
        
        elapsed = time.time() - start_time
        
        return jsonify({
            "response": response,
            "history": history,
            "elapsed_time": round(elapsed, 2)
        })
        
    except Exception as e:
        print(f"é”™è¯¯: {e}")
        return jsonify({"error": str(e)}), 500

if __name__ == '__main__':
    load_models()
    print("\nğŸŒ AIè¯¾ç¨‹åŠ©æ‰‹ RAG æœåŠ¡å™¨å¯åŠ¨ä¸­...")
    print("ğŸ“ è®¿é—®åœ°å€: http://localhost:6006")
    print("ğŸ“˜ RAGç«¯ç‚¹: POST /rag_chat (åŸºäºè¯¾ç¨‹çŸ¥è¯†åº“)")
    print("ğŸ’¬ æ™®é€šå¯¹è¯: POST /chat")
    print("â¤ï¸  å¥åº·æ£€æŸ¥: GET /health")
    app.run(host='0.0.0.0', port=6006, debug=False)
